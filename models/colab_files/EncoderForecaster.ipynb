{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMZo3tVDrsUi+e6YX7X5arK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from torch import nn\n","import torch\n","import logging\n","from collections import OrderedDict\n","import models.config as cfg"],"metadata":{"id":"0wRvzpt4bzM0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def make_layers(block):\n","  \"\"\"\n","  Function for creating layers used for preparing the input before feeding to ConvLSTM\n","  \"\"\"\n","  layers = []\n","  for layer_name, v in block.items():\n","    if 'conv' in layer_name:\n","      conv2d = nn.Conv2d(in_channels=v[0], out_channels=v[1],\n","                               kernel_size=v[2], stride=v[3],\n","                               padding=v[4])\n","      layers.append((layer_name, conv2d))\n","            ## Instead of using ReLU; here we used LeakyReLU\n","      layers.append(('leaky_' + layer_name, nn.LeakyReLU(negative_slope=0.2, inplace=True)))\n","    elif 'deconv' in layer_name:\n","      transposeConv2d = nn.ConvTranspose2d(in_channels=v[0], out_channels=v[1],\n","                                                 kernel_size=v[2], stride=v[3],\n","                                                 padding=v[4])\n","      layers.append((layer_name, transposeConv2d))\n","            ## Instead of using ReLU; here we used LeakyReLU\n","      layers.append(('leaky_' + layer_name, nn.LeakyReLU(negative_slope=0.2, inplace=True)))\n","    else:\n","      raise NotImplementedError\n","  return nn.Sequential(OrderedDict(layers))\n"],"metadata":{"id":"5yp2GnMhZU8l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xomze2dBY0ef"},"outputs":[],"source":["class Encoder(nn.Module):\n","  \"\"\"\n","  The class represent the Encoder part used for encode the input into features map used for Forecaster.\n","  \"\"\"\n","  def __init__(self, subnets, rnns):\n","    \"\"\"\n","    Initiate the Encoder.\n","    The parameter used here is Subnet and RNN.\n","\n","    Encoder consist of several stages which each of them consist of  Subnet and RNN Layers. \n","    Subnet used for preparing the input target into desired input matrix which further used in RNN.\n","    RNN processing the input for producinng the hidden state and output\n","    -----\n","    Parameter:\n","    - Subnet: list of layer used for preparing the input for the RNN\n","    - RNN: list of layer for predicting task\n","    \"\"\" \n","    super().__init__()\n","    # Check wheter the number of item in subnet equals to number of RNN layer\n","    assert len(subnets)==len(rnns)\n","    self.blocks = len(subnets)\n","\n","    # Initiate the stage and RNN layer\n","    for index, (params, rnn) in enumerate(zip(subnets, rnns), 1):\n","      # Stage represent the network used for preparing the input \n","      setattr(self, 'stage'+str(index), make_layers(params))\n","      # RNN represent the network used for processing the input feature\n","      setattr(self, 'rnn'+str(index), rnn)\n","\n","  def forward_by_stage(self, input, subnet, rnn):\n","    \"\"\"\n","    Function for proccessing the \"STAGE\" \n","    \"\"\"\n","    seq_number, batch_size, input_channel, height, width = input.size()\n","    input = torch.reshape(input, (-1, input_channel, height, width))\n","    # Preparing the input\n","    input = subnet(input)\n","    input = torch.reshape(input, (seq_number, batch_size, input.size(1), \n","                                      input.size(2), input.size(3)))\n","    # Processing the prepared input\n","    outputs_stage, state_stage = rnn(input, None)\n","    return outputs_stage, state_stage\n","\n","  # input: 5D S*B*I*H*W\n","  def forward(self, input):\n","    \"\"\"\n","    Feed forwarding the Encoding layer\n","    \"\"\"\n","    hidden_states = []\n","    logging.debug(input.size())\n","    for i in range(1, self.blocks+1):\n","      input, state_stage = self.forward_by_stage(input, getattr(self, 'stage'+str(i)), \n","                                                       getattr(self, 'rnn'+str(i)))\n","      hidden_states.append(state_stage)\n","    return tuple(hidden_states)"]},{"cell_type":"code","source":["class Forecaster(nn.Module):\n","  \"\"\"\n","  The class represent the Forecaster part used for forecasting the features map from Encoder\n","  \"\"\"\n","  def __init__(self, subnets, rnns):\n","    \"\"\"\n","    Initiate the Forecaster.\n","    The parameter used here is Subnet and RNN.\n","\n","    As similar with Encoder, Forecaster consist of several stages which each of them consist of  Subnet and RNN Layers. \n","    Contrary to the Encoder, here the flow is reversed from the RNN to Subnet. \n","    RNN processing the hidden state for producinng the hidden state and output\n","    Subnet used for translating the result into features which similar to the input.\n","    -----\n","    Parameter:\n","    - Subnet: list of layer used for tranlsation task\n","    - RNN: list of layer for predicting task\n","    \"\"\" \n","    super().__init__()\n","    # Check wheter the number of item in subnet equals to number of RNN layer\n","    assert len(subnets) == len(rnns)\n","    self.blocks = len(subnets)\n","\n","\n","    # Initiate the stage and RNN layer\n","    for index, (params, rnn) in enumerate(zip(subnets, rnns)):\n","      # RNN represent the network used for forecasting the hidden state\n","      setattr(self, 'rnn' + str(self.blocks-index), rnn)\n","      # Stage represent the network used for translation\n","      setattr(self, 'stage' + str(self.blocks-index), make_layers(params))\n","\n","  def forward_by_stage(self, input, state, subnet, rnn,seq_len=cfg.sequence_len['OUT']):\n","    \"\"\"\n","    Function for proccessing the \"STAGE\" \n","    \"\"\"\n","    # by default the number of predicting is set to 10\n","    # Forecasting task\n","    input, state_stage = rnn(input, state, seq_len)\n","    seq_number, batch_size, input_channel, height, width = input.size()\n","    input = torch.reshape(input, (-1, input_channel, height, width))\n","    # translation task\n","    input = subnet(input)\n","    input = torch.reshape(input, (seq_number, batch_size, input.size(1), input.size(2), input.size(3)))\n","\n","    return input\n","  \n","  # input: 5D S*B*I*H*W\n","  def forward(self, hidden_states):\n","    \"\"\"\n","    Feed forwarding the Encoding layer\n","    \"\"\"\n","    input = self.forward_by_stage(None, hidden_states[-1], getattr(self, 'stage'+str(self.blocks)),\n","                                      getattr(self, 'rnn'+str(self.blocks)))\n","    for i in list(range(1, self.blocks))[::-1]:\n","      input = self.forward_by_stage(input, hidden_states[i-1], getattr(self, 'stage' + str(i)),\n","                                                       getattr(self, 'rnn' + str(i)))\n","    return input\n"],"metadata":{"id":"u0u68jIKZRsM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EF(nn.Module):\n","  \"\"\"\n","  The class represent the encapsulation of Encoder part and Decoder part\n","  \"\"\"\n","  def __init__(self, encoder, forecaster):\n","    super().__init__()\n","    self.encoder = encoder\n","    self.forecaster = forecaster\n","\n","  def forward(self, input):\n","    state = self.encoder(input)\n","    output = self.forecaster(state)\n","    return output"],"metadata":{"id":"xmZ-PPKmuPhc"},"execution_count":null,"outputs":[]}]}